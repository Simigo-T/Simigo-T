import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import re
import string
from sklearn.metrics import confusion_matrix, accuracy_score
import lime
from lime import lime_text
import lime.lime_text
from sklearn.feature_extraction.text import TfidfVectorizer



# The orginal dataset
original_data_path = "Training_Essay_Data.csv"
df = pd.read_csv(original_data_path)

#Hardly paraphrased text By ChatGPT
original_data_path = "Attacked_Model_Text_Training_Essay_Data.csv"
df3 = pd.read_csv(original_data_path)

# The softly paraphrased text by ChatGPT
high_pharaphrased_data_path = "Deeply_Humanized_Training_Essay_Data.csv"
df2=pd.read_csv(high_pharaphrased_data_path)

# Medium paraphrased text by ChatGPT
high_pharaphrased_data_path = "Extensively_Humanized_Training_Essay_Data.csv"
df1=pd.read_csv(high_pharaphrased_data_path)

# The datsets below could be adapted after the code to get the accuracy for the datasets paraphrased by WordSpinner. 
'''
#WSO: The original sample of 10 human-written and 10 AI-written texts: WS0 
deep_pharaphrased_data_path = "Updated_AI_and_Human_Texts.csv" #Word spinner
df1=pd.read_csv(deep_pharaphrased_data_path)

# First iteration of pharaphrased from WSO: 
deep_pharaphrased_data_path = "Wordspinner2.csv" #Word spinner
df1=pd.read_csv(deep_pharaphrased_data_path)

# Second iteration of the file 
deep_pharaphrased_data_path = "wordspinner3.csv" #Word spinner
df2=pd.read_csv(deep_pharaphrased_data_path)

# The third pharaphrsing iteration on WordSpinner
deep_pharaphrased_data_path = "wordspinner3.csv" #Word spinner
df2=pd.read_csv(deep_pharaphrased_data_path)
'''

x = df["text"]   # Define independet varible
y = df["generated"] # Defining dependent varible
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,random_state=42)  # Splitting to train and test data


x1 = df1["text"]   # Define independet varible
y1 = df1["generated"] # Defining dependent varible
  # Splitting to train and test data

x2 = df2["text"]   # Define independet varible
y2 = df2["generated"] # Defining dependent varible
  # Splitting to train and test data

x3 = df3["text"]   # Define independet varible
y3 = df3["generated"] # Defining dependent varible
  # Splitting to train and test data


from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=1000)
x_train_vec = tfidf_vectorizer.fit_transform(x_train)  # Fit on training data
x_test_vec = tfidf_vectorizer.transform(x_test)        # Transform training test data

x1_vec = tfidf_vectorizer.transform(x1) 
x2_vec=tfidf_vectorizer.transform(x2)
x3_vec=tfidf_vectorizer.transform(x3)

from sklearn.ensemble import RandomForestClassifier

RFC = RandomForestClassifier(n_estimators=100,random_state=42)
RFC.fit(x_train_vec, y_train)

y_pred = RFC.predict(x_test_vec)
y1_pred = RFC.predict(x1_vec)
y2_pred = RFC.predict(x2_vec)
y3_pred= RFC.predict(x3_vec)

conf_matrix = confusion_matrix(y_test, y_pred)
conf_matrix1 = confusion_matrix(y1, y1_pred)
conf_matrix2 = confusion_matrix(y2, y2_pred)
conf_matrix3 = confusion_matrix(y3, y3_pred)

accuracy = accuracy_score(y_test, y_pred)
accuracy1 = accuracy_score(y1, y1_pred)
accuracy2 = accuracy_score(y2, y2_pred)
accuracy3 = accuracy_score(y3, y3_pred)


