import pandas as pd
import numpy as np
import lime
from lime import lime_text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from IPython.core.display import display, HTML
import matplotlib.pyplot as plt

# File paths For the orginal and paraphrased datasets using Wordspinner.
standard_data_path = "Training_Essay_Data.csv"
ai_human_data_path = "Updated_AI_and_Human_Texts.csv" # First iteration of WordSpinner
wordspinner3_path = "wordspinner2.csv"                # Second iteration
wordspinner4_path = "wordspinner3.csv"                # Third iteration

# Loading the datasets
standard_df = pd.read_csv(standard_data_path)
ai_human_df = pd.read_csv(ai_human_data_path)
wordspinner3_df = pd.read_csv(wordspinner3_path)
wordspinner4_df = pd.read_csv(wordspinner4_path)

# Extract text and labels
datasets = {
    "Standard": (standard_df["text"], standard_df["generated"]),
    "AI_and_Human": (ai_human_df["text"], ai_human_df["generated"]),
    "Wordspinner3": (wordspinner3_df["text"], wordspinner3_df["generated"]),
    "Wordspinner4": (wordspinner4_df["text"], wordspinner4_df["generated"]),
}

# We are splitting the original datasets into traning and test data. 
x_train, x_test, y_train, y_test = train_test_split(datasets["Standard"][0], datasets["Standard"][1], test_size=0.2, random_state=42)

# Vectorization and transformation of the data.
tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=1000)
x_train_vec = tfidf_vectorizer.fit_transform(x_train)
x_test_vec = tfidf_vectorizer.transform(x_test)     

vectorized_datasets = {name: (tfidf_vectorizer.transform(data[0]), data[1]) for name, data in datasets.items()}

# Traning the Random forest
RFC = RandomForestClassifier(n_estimators=100, random_state=42)
RFC.fit(x_train_vec, y_train)

# We are defining the Lime Explanier
explainer = lime.lime_text.LimeTextExplainer(class_names=['Human', 'AI'])

def predict_fn(texts):
    return RFC.predict_proba(tfidf_vectorizer.transform(texts))

# Function for the Explanation of  a given datasets and indices passed through. 
def generate_lime_explanation(dataset_name, text_idx, text_series):
    text_example = text_series.iloc[text_idx]
    explanation = explainer.explain_instance(text_example, predict_fn, num_features=10)
    
    # Save explanation as an HTML file
    html_explanation = explanation.as_html()
    with open(f'lime_explanation_{dataset_name}_Index_{text_idx}.html', 'w') as f:
        f.write(html_explanation)
    print(f"LIME explanation saved as 'lime_explanation_{dataset_name}_Index_{text_idx}.html'")
    
    # Displaying in HTML form
    display(HTML(html_explanation))

# choosing a index where the essays are same to compare between them.
indices = {
    "Standard": 2555,
    "AI_and_Human": 7,
    "Wordspinner3": 7,
    "Wordspinner4": 7,
}

# This generates all the explanations for a iven datasets
for dataset_name, (texts, _) in datasets.items():
    try:
        generate_lime_explanation(dataset_name, indices[dataset_name], texts)
    except IndexError:
        print(f"Index {indices[dataset_name]} out of range for dataset '{dataset_name}'")


# In order to perform LIME Explanations for the paraphrased datasets using CHATGPT, The following code could be adapted after: 

'''
#Hardly paraphrased text By ChatGPT
original_data_path = "Attacked_Model_Text_Training_Essay_Data.csv"
df3 = pd.read_csv(original_data_path)

# The softly paraphrased text by ChatGPT
high_pharaphrased_data_path = "Deeply_Humanized_Training_Essay_Data.csv"
df2=pd.read_csv(high_pharaphrased_data_path)

# Medium paraphrased text by ChatGPT
high_pharaphrased_data_path = "Extensively_Humanized_Training_Essay_Data.csv"
df1=pd.read_csv(high_pharaphrased_data_path)

'''


